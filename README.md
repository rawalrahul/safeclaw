# SafeClaw â€” Secure Personal AI Assistant

**Sleep-by-default. Tools off by default. You hold the keys.**

SafeClaw is a privacy-first AI assistant you control from your phone via Telegram. You connect it to your own LLM API key (Anthropic Claude, OpenAI GPT, or Google Gemini). It auto-discovers tools from any MCP servers you've configured for Claude Code.

Unlike always-on AI gateways, SafeClaw inverts the defaults:

| Problem with most AI gateways | SafeClaw's answer |
|-------------------------------|-------------------|
| Bot hijacks your messaging account | **Own identity** â€” SafeClaw is its own Telegram bot, never impersonates you |
| Always-on with a large attack surface | **Dormant by default** â€” only wakes when you send `/wake` |
| Static tool permissions set in config | **Runtime toggle** â€” `/enable browser`, `/disable shell`, on the fly |
| Dangerous actions execute immediately | **Explicit approval** â€” every write, delete, execute requires `/confirm` |
| Unknown senders get error responses | **Silent drop** â€” non-owners receive zero response, zero acknowledgment |
| LLM tools locked to a fixed list | **MCP auto-discovery** â€” picks up any MCP server from your Claude settings |

---

## Prerequisites

- **Node.js 22+** (`node --version` to check)
- A **Telegram account** (to talk to the bot)
- An API key from at least one LLM provider, **or** a locally running Ollama instance:
  - **Anthropic** â€” [console.anthropic.com](https://console.anthropic.com) â†’ API Keys
  - **OpenAI** â€” [platform.openai.com](https://platform.openai.com) â†’ API Keys
  - **Google Gemini** â€” [aistudio.google.com](https://aistudio.google.com) â†’ Get API Key *(free tier available)*
  - **Ollama** â€” free, runs entirely on your machine, no API key required (see below)

---

## Step 1 â€” Create a Telegram Bot

1. Open Telegram and message **[@BotFather](https://t.me/BotFather)**
2. Send `/newbot` and follow the prompts (pick any name and username)
3. Copy the **bot token** â€” looks like `7412345678:AAFz...`

> Keep this token private. Anyone with it can control your bot.

---

## Step 2 â€” Find Your Telegram User ID

1. Message **[@userinfobot](https://t.me/userinfobot)** on Telegram
2. It replies with your numeric user ID â€” looks like `123456789`

This ID is how SafeClaw knows you're the owner. Every message from any other ID is silently dropped.

---

## Step 3 â€” Install SafeClaw

```bash
git clone https://github.com/yourname/safeclaw.git
cd safeclaw
npm install
```

---

## Step 4 â€” Configure

```bash
cp .env.example .env
```

Edit `.env`:

```env
TELEGRAM_BOT_TOKEN=7412345678:AAFz...       # from BotFather
OWNER_TELEGRAM_ID=123456789                 # your numeric Telegram ID
INACTIVITY_TIMEOUT_MINUTES=30               # optional, default 30
WORKSPACE_DIR=/home/you/safeclaw-workspace  # optional, default ~/safeclaw-workspace
```

> `WORKSPACE_DIR` is the only directory SafeClaw's filesystem tool can read or write. It is sandboxed â€” paths that try to escape it (e.g. `../../etc/passwd`) are rejected.

---

## Step 5 â€” Run

```bash
# Development (auto-reloads on file changes)
npm run dev

# Production
npm start
```

You should see:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SafeClaw â€” Secure AI Assistant     â”‚
â”‚   Sleep-by-default. You hold the keysâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[telegram] Bot online: @YourBotName
[telegram] Send /wake from your Telegram to activate
Security status:
  âœ“ Gateway: DORMANT (ignoring all messages except /wake)
  âœ“ Tools: ALL DISABLED
  âœ“ Authentication: single-owner (Telegram ID)
  âœ“ Owner: 123456789
```

---

## Step 6 â€” Connect to an LLM

Open Telegram, find your bot, and store your API key. **These commands work even while the gateway is dormant** â€” you don't need to `/wake` first.

### Anthropic Claude

```
/auth anthropic sk-ant-api03-...
```

Default model: `claude-sonnet-4-5-20250929`

### OpenAI GPT

```
/auth openai sk-proj-...
```

Default model: `gpt-4o`

### Google Gemini (free tier available)

```
/auth gemini AIza...
```

Default model: `gemini-2.0-flash`. Get a free key at [aistudio.google.com](https://aistudio.google.com) â€” no billing required.

### Ollama (local LLM â€” no API key needed)

Ollama lets you run open-source LLMs entirely on your own machine. No cloud account, no usage costs, full privacy.

#### Step A â€” Install Ollama

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Download the installer from https://ollama.com/download
```

#### Step B â€” Pull a model

```bash
ollama pull llama3.2          # recommended â€” fast, supports tool calling
ollama pull qwen2.5           # strong alternative with tool calling
ollama pull mistral-nemo      # good balance of speed and quality
```

> **Tool calling note:** SafeClaw's LLM agent relies on structured tool calling. Use models known to support it: `llama3.1`, `llama3.2`, `qwen2.5`, `mistral-nemo`. General chat models (`phi3`, `gemma`, etc.) may respond without tool calls even when tools are enabled.

#### Step C â€” Start the Ollama server

```bash
ollama serve
```

Ollama binds to `http://localhost:11434` by default. Leave this running while SafeClaw is active.

#### Step D â€” Register Ollama with SafeClaw

In Telegram, tell SafeClaw where Ollama is running. **This works even while the gateway is dormant.**

```
/auth ollama local
```

`local` is a shorthand for `http://localhost:11434`. If Ollama is on another machine or a different port, pass the full URL:

```
/auth ollama http://192.168.1.50:11434
```

#### Step E â€” Select a model

```
/model ollama/llama3.2
```

Or list all models currently installed in your Ollama instance:

```
/model list ollama
```

Example output:
```
Active: ollama / llama3.2

ollama â€” 3 model(s):
  â–¶ llama3.2
    qwen2.5
    mistral-nemo

Switch with: /model <provider>/<model-id>
```

#### Running Ollama on a remote machine / GPU server

If you have a machine with a GPU, you can run Ollama there and point SafeClaw at it:

```bash
# On the GPU server â€” bind to all interfaces
OLLAMA_HOST=0.0.0.0 ollama serve
```

```
# In Telegram
/auth ollama http://<server-ip>:11434
/model ollama/llama3.3
```

---

### Check what's configured

```
/auth status
```

Output example:
```
Auth Status:
  Active: anthropic / claude-sonnet-4-5-20250929

  Providers:
    anthropic: sk-ant-ap...a1b2  (active)
    openai: not configured
    gemini: AIzaSy...x9y8
    ollama: http://localhost:11434
```

### Remove a stored API key

```
/auth remove anthropic
/auth remove openai
/auth remove gemini
/auth remove ollama
```

If you remove the active provider, SafeClaw automatically switches to another configured one. If it was the last key, the active provider is cleared and you'll need to add a new one before the LLM agent can respond.

---

### Browse and switch models

`/model` fetches the live model list directly from each provider's API â€” no hardcoded lists to go stale.

```
/model                          â†’ list all models for every configured provider
/model list anthropic           â†’ list only Anthropic models
/model list openai              â†’ list only OpenAI models
/model list gemini              â†’ list only Gemini models
```

Example output:
```
Active: anthropic / claude-sonnet-4-5-20250929

anthropic â€” 6 model(s):
  â–¶ claude-sonnet-4-5-20250929  (Claude Sonnet 4.5)
    claude-opus-4-6  (Claude Opus 4.6)
    claude-haiku-4-5-20251001  (Claude Haiku 4.5)
    ...

gemini â€” 12 model(s):
    gemini-2.0-flash  (Gemini 2.0 Flash)
    gemini-1.5-pro  (Gemini 1.5 Pro)
    ...

Switch with: /model <provider>/<model-id>
```

To switch:
```
/model anthropic/claude-opus-4-6
/model openai/gpt-4o-mini
/model gemini/gemini-1.5-pro
```

Credentials are stored in `~/.safeclaw/auth.json`. They persist across restarts.

---

## Step 7 â€” Wake Up and Go

```
/wake
```

The bot replies with available commands and the auto-sleep timeout. Now you can:

```
/enable filesystem       â†’ allow file reads and writes
/enable browser          â†’ allow web browsing
/tools                   â†’ see everything and its ON/OFF status
```

Then just talk naturally:

```
You:  what files are in my workspace?
Bot:  [lists files]

You:  write a Python script that fetches the Hacker News front page
Bot:  Action pending approval:
        Tool: filesystem/write_file
        Details: write_file: hn_fetch.py (312 chars)
        Expires in: 300s
      Reply /confirm a1b2c3d4 or /deny a1b2c3d4

You:  /confirm a1b2c3d4
Bot:  Approved. [LLM follow-up: "Done! I've written hn_fetch.py ..."]
```

---

## All Commands

### Lifecycle

| Command | Works dormant? | Description |
|---------|---------------|-------------|
| `/wake` | Yes | Wake the gateway |
| `/sleep` | No | Return to dormant, disconnect MCP servers |
| `/kill` | No | Emergency shutdown, stops the process |

### LLM Provider Setup

| Command | Works dormant? | Description |
|---------|---------------|-------------|
| `/auth <provider> <api-key>` | Yes | Store API key (`anthropic`, `openai`, `gemini`) or Ollama URL (`/auth ollama local`) |
| `/auth status` | Yes | Show all configured providers and the active one |
| `/auth remove <provider>` | Yes | Delete a stored API key |
| `/model` | Yes | List all available models fetched live from provider APIs |
| `/model list <provider>` | Yes | List models for one specific provider |
| `/model <provider/model>` | Yes | Switch to a specific model |

### Tools

| Command | Description |
|---------|-------------|
| `/tools` | List all tools (builtin + MCP) with ON/OFF status |
| `/enable <tool>` | Enable a builtin tool |
| `/disable <tool>` | Disable a builtin tool |
| `/enable mcp:<server>` | Enable all tools for an MCP server |
| `/disable mcp:<server>` | Disable all tools for an MCP server |

**Builtin tools:** `browser`, `filesystem`, `shell`, `code_exec`, `network`, `messaging`

### Permissions

| Command | Description |
|---------|-------------|
| `/confirm <id>` | Approve a pending dangerous action |
| `/deny <id>` | Reject a pending action |
| `/confirm` | List all pending approvals with their IDs |

### Info

| Command | Description |
|---------|-------------|
| `/status` | Gateway state, uptime, idle time, enabled tools |
| `/audit [n]` | Last N audit log events (default 10) |
| `/help` | All commands inline in Telegram |

---

## Self-Extending Skills

SafeClaw can detect when it lacks a capability and propose new skills at runtime â€” without a restart.

### How it works

When you give the agent a task it can't complete with its current tools (e.g. "create a PDF report"), it automatically:

1. Recognises the capability gap
2. Generates a working JavaScript implementation
3. Sends you a proposal with a full code preview for review
4. Waits for your `/confirm` before installing anything

On approval, the skill is written to `~/.safeclaw/skills/<name>.mjs`, dynamically imported, and immediately available. It persists across restarts.

### Example session

```
You:  create a PDF summary of my workspace notes

Bot:  ğŸ”§ Skill Proposal: pdf_create
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      Description: Create PDF documents from text content
      Needed for: Generating a PDF summary of workspace notes

      âš ï¸  This skill performs potentially dangerous operations (file writes, network calls, etc.).

      Proposed code:
      ```
      import { writeFileSync } from "node:fs";
      import { join } from "node:path";

      export const skill = {
        name: "pdf_create",
        description: "Create PDF documents from text content",
        dangerous: true,
        parameters: {
          type: "object",
          properties: {
            filename: { type: "string" },
            content: { type: "string" }
          },
          required: ["filename", "content"]
        },
        async execute(params) {
          // Simple text file fallback if pdfkit not available
          const path = join(process.env.WORKSPACE_DIR || ".", params.filename);
          writeFileSync(path, params.content, "utf8");
          return `Saved to ${params.filename}`;
        }
      };
      ```

      âš ï¸  This code will run inside the SafeClaw process with full Node.js access.
      Review it carefully before approving.

      Expires in: 300s

      /confirm a1b2c3d4  â†’  install skill
      /deny a1b2c3d4     â†’  reject proposal

You:  /confirm a1b2c3d4

Bot:  Approved.
      Skill "pdf_create" has been installed and is now active.
      [Agent continues and creates the PDF...]
```

### Skill storage

Skills live in `~/.safeclaw/skills/` as ES module JavaScript files (`.mjs`). They load on every `/wake`. You can inspect or delete them directly:

```bash
ls ~/.safeclaw/skills/
cat ~/.safeclaw/skills/pdf_create.mjs
rm ~/.safeclaw/skills/pdf_create.mjs   # remove a skill permanently
```

### Skill tool names

Installed skills appear in `/tools` under **Dynamic Skills** and are referenced with a `skill__` prefix:

```
/tools
â†’  Dynamic Skills (installed by agent):
     OFF  skill__pdf_create â€” Create PDF documents from text content âš ï¸

/enable skill__pdf_create
/disable skill__pdf_create
```

### Security

- **Every skill install requires `/confirm`** â€” same as any other dangerous action
- **Full code is shown before you approve** â€” never a black box
- **Skills run with full Node.js access** â€” treat them like running a shell script you wrote yourself
- **Skills are disabled by default after install** â€” the agent auto-enables the freshly installed skill for the current session, but after a `/sleep` + `/wake` cycle you control whether it's enabled via `/enable skill__<name>`

---

## MCP Tool Auto-Discovery

SafeClaw reads the MCP server configuration from your Claude Code settings and automatically discovers all tools from them on every `/wake`.

### How it works

1. You send `/wake`
2. SafeClaw immediately replies (it doesn't block on MCP discovery)
3. In the background it reads `~/.claude/settings.json` â†’ `mcpServers`
4. For each configured server it connects, calls `listTools()`, and registers the results
5. Run `/tools` a moment later to see the discovered tools grouped by server
6. Tools are classified as safe or dangerous by keyword heuristics (read/get/list â†’ safe; write/delete/create/send â†’ dangerous)

### Enabling MCP tools

```
/enable mcp:my-server      â†’ enable all tools for "my-server"
/disable mcp:my-server     â†’ disable all tools for "my-server"
/tools                     â†’ see full list including MCP tools
```

### Example `~/.claude/settings.json`

```json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_TOKEN": "${GITHUB_TOKEN}"
      }
    }
  }
}
```

Environment variable placeholders like `${GITHUB_TOKEN}` are resolved from the process environment. Servers that fail to connect (wrong command, network error, 401/403 auth) are skipped with a console warning â€” they don't crash the bot.

> **Note:** HTTP/SSE MCP servers are not yet supported. Only `stdio` servers (those with a `command` field) are connected.

---

## A Realistic Session

```
[Phone]

You:   /wake
Bot:   Gateway awake. All tools are disabled by default.
       Auto-sleep in 30 minutes of inactivity.
       Use /tools to see tools, /enable <tool> to activate one.

You:   /tools
Bot:   Tool Registry:
         OFF  browser â€” Web browsing and search
         OFF  filesystem â€” Read, write, and delete files
         OFF  shell â€” Execute shell commands
         OFF  code_exec â€” Run code snippets
         OFF  network â€” HTTP requests and API calls
         OFF  messaging â€” Send messages to contacts

       MCP Servers:
         github:
           OFF  mcp__github__search_repositories â€” Search GitHub repositories
           OFF  mcp__github__create_issue â€” Create a GitHub issue
           OFF  mcp__github__get_file_contents â€” Get file contents from a repo

You:   /enable filesystem
Bot:   filesystem is now ENABLED.
       Dangerous actions will still require /confirm before executing.

You:   /enable mcp:github
Bot:   Enabled 3 tool(s) for MCP server "github".

You:   search for typescript MCP server examples on GitHub
Bot:   Found 12 repositories matching "typescript MCP server":
       1. modelcontextprotocol/typescript-sdk â€” ...
       2. ...

You:   write those results to a file called mcp-examples.md
Bot:   Action pending approval:
         Details: write_file: mcp-examples.md (1.4 KB)
         Expires in: 300s
       Reply /confirm 9f2e1a8b or /deny 9f2e1a8b

You:   /confirm 9f2e1a8b
Bot:   Approved.
       Done! I've saved the search results to mcp-examples.md in your workspace.

You:   /sleep
Bot:   Gateway dormant. Goodnight.
```

---

## Security Model

- **Sleep-by-default**: Gateway starts dormant and ignores all messages except `/wake` from the owner
- **Single owner**: Only your Telegram user ID is authorized. Everyone else is silently ignored
- **Tools off by default**: All tools â€” builtin and MCP â€” are disabled on every wake
- **Confirm before dangerous action**: Write, delete, execute, send, and create operations require `/confirm`
- **Auto-sleep**: Inactivity timeout (default 30 min) returns to dormant automatically
- **Full audit trail**: Every event logged to `~/.safeclaw/audit.jsonl`
- **Separate identity**: The bot is its own Telegram account, never acts as you
- **MCP isolation**: Each MCP server runs as a subprocess; crashing servers don't crash SafeClaw
- **Workspace sandboxing**: Filesystem tool is restricted to `WORKSPACE_DIR` â€” no escape via `../`

---

## Project Structure

```
safeclaw/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts                  # Entry point and startup
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ types.ts              # All TypeScript interfaces and enums
â”‚   â”‚   â”œâ”€â”€ gateway.ts            # State machine (dormant/awake/action_pending)
â”‚   â”‚   â”œâ”€â”€ auth.ts               # Single-owner Telegram ID check
â”‚   â”‚   â””â”€â”€ config.ts             # .env loader and config validation
â”‚   â”œâ”€â”€ channels/telegram/
â”‚   â”‚   â”œâ”€â”€ client.ts             # grammy bot setup
â”‚   â”‚   â”œâ”€â”€ handler.ts            # Inbound routing, auth check, command dispatch
â”‚   â”‚   â”œâ”€â”€ sender.ts             # Outbound with message chunking for long replies
â”‚   â”‚   â””â”€â”€ free-text.ts          # Routes plain messages to LLM agent
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ types.ts              # LLMProvider interface, ProviderName, defaults
â”‚   â”‚   â”œâ”€â”€ anthropic.ts          # Anthropic Claude API client
â”‚   â”‚   â”œâ”€â”€ openai.ts             # OpenAI API client
â”‚   â”‚   â”œâ”€â”€ gemini.ts             # Google Gemini API client
â”‚   â”‚   â”œâ”€â”€ ollama.ts             # Ollama local LLM client (OpenAI-compatible)
â”‚   â”‚   â”œâ”€â”€ models.ts             # Live model listing from provider APIs
â”‚   â”‚   â”œâ”€â”€ store.ts              # Persists API keys to ~/.safeclaw/auth.json
â”‚   â”‚   â””â”€â”€ resolver.ts           # Picks the active provider and model
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ session.ts            # Conversation history (trimmed to avoid token limits)
â”‚   â”‚   â”œâ”€â”€ tool-schemas.ts       # Builtin tool schemas + MCP schema passthrough
â”‚   â”‚   â””â”€â”€ runner.ts             # LLM loop: execute safe tools, queue dangerous ones
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ registry.ts           # Tool map: enable/disable, MCP register/clear
â”‚   â”‚   â”œâ”€â”€ executor.ts           # Dispatches to real impl or MCP callTool
â”‚   â”‚   â”œâ”€â”€ filesystem.ts         # Real fs: read, list, write, delete (sandboxed)
â”‚   â”‚   â”œâ”€â”€ browser.ts            # Stub (simulated response)
â”‚   â”‚   â”œâ”€â”€ shell.ts              # Stub
â”‚   â”‚   â”œâ”€â”€ messaging.ts          # Stub
â”‚   â”‚   â”œâ”€â”€ code_exec.ts          # Stub
â”‚   â”‚   â””â”€â”€ network.ts            # Stub
â”‚   â”œâ”€â”€ skills/
â”‚   â”‚   â”œâ”€â”€ dynamic.ts            # DynamicSkill interface + .mjs file loader
â”‚   â”‚   â””â”€â”€ manager.ts            # SkillsManager: install, load, persist skills
â”‚   â”œâ”€â”€ mcp/
â”‚   â”‚   â”œâ”€â”€ config.ts             # Reads ~/.claude/settings.json mcpServers block
â”‚   â”‚   â”œâ”€â”€ manager.ts            # Connect/discover/call/disconnect MCP servers
â”‚   â”‚   â””â”€â”€ index.ts              # Barrel export
â”‚   â”œâ”€â”€ permissions/
â”‚   â”‚   â””â”€â”€ store.ts              # Pending approval store with 5-min expiry
â”‚   â”œâ”€â”€ audit/
â”‚   â”‚   â””â”€â”€ logger.ts             # JSONL event logger to ~/.safeclaw/audit.jsonl
â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”œâ”€â”€ parser.ts             # /command tokenizer
â”‚   â”‚   â””â”€â”€ handlers.ts           # Handler for each command
â”‚   â””â”€â”€ storage/
â”‚       â””â”€â”€ persistence.ts        # JSON/JSONL read-write helpers
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â””â”€â”€ CLAUDE.md                     # Architecture and dev guidelines
```

---

## What's Real vs. Simulated

### Real (fully implemented)
- Gateway state machine with all transitions (dormant / awake / action_pending / shutdown)
- Single-owner authentication with silent drop for unauthorized senders
- Tool registry with runtime enable/disable
- Permission confirmation flow with 5-minute expiry
- JSONL audit logging to `~/.safeclaw/audit.jsonl`
- Telegram bot integration (grammy)
- LLM agent â€” **Anthropic Claude, OpenAI GPT, Google Gemini, and Ollama (local)** with real tool_use/function calling
- Live model listing fetched from provider APIs (`/model`)
- Persistent API key storage in `~/.safeclaw/auth.json`
- **Real filesystem** â€” read, write, list, delete (sandboxed to `WORKSPACE_DIR`)
- **MCP auto-discovery** â€” stdio servers from `~/.claude/settings.json`
- **Self-extending skills** â€” agent proposes new capabilities, owner approves, skills are dynamically installed and loaded without restart
- Conversation sessions with history trimming

### Simulated (stub responses)
- `browser` â€” returns mock search results
- `shell` â€” returns mock command output
- `messaging` â€” pretends to send a message
- `code_exec` â€” returns mock execution result
- `network` â€” returns mock HTTP response

These stubs demonstrate the full permission flow safely. Replacing them with real implementations is straightforward â€” see the pattern in `src/tools/filesystem.ts`.

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Runtime | Node.js 22+ |
| Language | TypeScript 5 (strict mode) |
| Telegram | [grammy](https://grammy.dev) |
| LLM: Anthropic | Anthropic Messages API (raw fetch) |
| LLM: OpenAI | OpenAI Chat Completions API (raw fetch) |
| LLM: Gemini | Google Generative Language API (raw fetch) |
| LLM: Ollama | Ollama OpenAI-compatible API (local, raw fetch) |
| MCP | `@modelcontextprotocol/sdk` ^1.12.0 |
| Storage | File-based JSON + JSONL (no database) |
| Dev runner | `tsx` (TypeScript execute, no build step needed) |

---

## Roadmap

- [x] Gateway state machine, Telegram integration, commands, audit
- [x] Real filesystem tools with path sandboxing
- [x] LLM agent â€” Anthropic Claude, OpenAI GPT, Google Gemini, and Ollama (local) â€” with tool calling
- [x] Live model listing fetched from provider APIs (+ local listing for Ollama)
- [x] MCP tool auto-discovery from `~/.claude/settings.json`
- [x] Self-extending skills â€” agent proposes and installs new capabilities at runtime
- [ ] Real browser tool (Playwright / Puppeteer)
- [ ] Real shell execution (with timeout and output limits)
- [ ] HTTP/SSE MCP server support
- [ ] WhatsApp Cloud API integration
- [ ] Web dashboard for visual tool management
- [ ] Multi-owner support with invite codes
- [ ] Rate limiting per tool per session

---

## Contributing

Contributions welcome! Please open an issue first to discuss what you'd like to change. The architecture is documented in `CLAUDE.md`.

## License

[MIT](LICENSE)
