# SafeClaw ‚Äî Secure Personal AI Assistant

**Sleep-by-default. Tools off by default. You hold the keys.**

SafeClaw is a privacy-first AI assistant you control from your phone via Telegram. You connect it to your own LLM API key (Anthropic Claude, OpenAI GPT, Google Gemini, or a local Ollama instance). It auto-discovers tools from any MCP servers you've configured, adapts to your available hardware, and orchestrates multi-step tasks using a manager‚Äìworker‚Äìreviewer agent pipeline ‚Äî all without any telemetry, cloud accounts, or third-party data handling.

Unlike always-on AI gateways, SafeClaw inverts the defaults:

| Problem with most AI gateways | SafeClaw's answer |
|-------------------------------|-------------------|
| Bot hijacks your messaging account | **Own identity** ‚Äî SafeClaw is its own Telegram bot |
| Always-on with a large attack surface | **Dormant by default** ‚Äî only wakes on `/wake` |
| Static tool permissions set in config | **Runtime toggle** ‚Äî `/enable browser`, `/disable shell`, on the fly |
| Dangerous actions execute immediately | **Explicit approval** ‚Äî every write, delete, execute requires `/confirm` |
| Unknown senders get error responses | **Silent drop** ‚Äî non-owners receive zero response, zero acknowledgment |
| LLM can read your API keys and `.env` | **SecretGuard** ‚Äî protected paths blocked at the tool layer, never reach the LLM |
| Agent ignores hardware constraints | **Infra-aware** ‚Äî probes CPU/RAM/GPU/Ollama on wake, calibrates worker count |
| One LLM handles everything linearly | **Multi-agent** ‚Äî manager decomposes complex tasks into parallel/sequential workers |

---

## Prerequisites

- **Node.js 22+** (`node --version` to check)
- A **Telegram account** (to talk to the bot)
- At least one of:
  - **Anthropic** API key ‚Äî [console.anthropic.com](https://console.anthropic.com) ‚Üí API Keys
  - **OpenAI** API key ‚Äî [platform.openai.com](https://platform.openai.com) ‚Üí API Keys
  - **Google Gemini** API key ‚Äî [aistudio.google.com](https://aistudio.google.com) ‚Üí Get API Key *(free tier available)*
  - **Ollama** ‚Äî free, runs entirely on your machine, no API key required (see below)

---

## Step 1 ‚Äî Create a Telegram Bot

1. Open Telegram and message **[@BotFather](https://t.me/BotFather)**
2. Send `/newbot` and follow the prompts
3. Copy the **bot token** ‚Äî looks like `7412345678:AAFz...`

> Keep this token private. Anyone with it can control your bot.

---

## Step 2 ‚Äî Find Your Telegram User ID

1. Message **[@userinfobot](https://t.me/userinfobot)** on Telegram
2. It replies with your numeric user ID ‚Äî looks like `123456789`

This ID is how SafeClaw knows you're the owner. Every message from any other ID is silently dropped.

---

## Step 3 ‚Äî Install SafeClaw

```bash
git clone https://github.com/yourname/safeclaw.git
cd safeclaw
npm install
```

---

## Step 4 ‚Äî Configure

SafeClaw reads Telegram credentials from **`~/.safeclaw/telegram.json`** (recommended ‚Äî outside the project directory, protected by SecretGuard). On first run it can also read them from `.env` and auto-migrates them.

### Option A ‚Äî Recommended: create `~/.safeclaw/telegram.json`

```bash
mkdir -p ~/.safeclaw
```

Create `~/.safeclaw/telegram.json`:

```json
{
  "botToken": "7412345678:AAFz...",
  "ownerTelegramId": 123456789
}
```

Then start SafeClaw. No `.env` needed for the token ‚Äî it loads directly from there on every start.

### Option B ‚Äî First-run via `.env` (auto-migrates)

```bash
cp .env.example .env
```

Edit `.env`:

```env
TELEGRAM_BOT_TOKEN=7412345678:AAFz...       # from BotFather
OWNER_TELEGRAM_ID=123456789                 # your numeric Telegram ID
```

On first start, SafeClaw **automatically** saves these to `~/.safeclaw/telegram.json` and prints:

```
[config] Telegram credentials saved to ~/.safeclaw/telegram.json
[config] You can now remove TELEGRAM_BOT_TOKEN and OWNER_TELEGRAM_ID from .env
```

After that, clear the token from `.env` ‚Äî future starts load from `telegram.json`.

### Optional tunables (`.env` only)

```env
INACTIVITY_TIMEOUT_MINUTES=30               # default 30
WORKSPACE_DIR=/home/you/safeclaw-workspace  # default ~/safeclaw-workspace
```

> `WORKSPACE_DIR` is the only directory SafeClaw's filesystem tool can read or write. Paths that try to escape it (e.g. `../../etc/passwd`) are rejected. `.env` files and all `~/.safeclaw/*.json` files (including `telegram.json`) are blocked by SecretGuard ‚Äî the LLM can never read them.

---

## Step 5 ‚Äî Run

```bash
# Development (auto-reloads on file changes)
npm run dev

# Production
npm start
```

You should see:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   SafeClaw ‚Äî Secure AI Assistant     ‚îÇ
‚îÇ   Sleep-by-default. You hold the keys‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
[config] Owner Telegram ID: 123456789
[config] Storage: /home/you/.safeclaw
[config] Inactivity timeout: 30min
[telegram] Bot online: @YourBotName
[telegram] Send /wake from your Telegram to activate

Security status:
  ‚úì Gateway: DORMANT (ignoring all messages except /wake)
  ‚úì Tools: ALL DISABLED
  ‚úì Authentication: single-owner (Telegram ID)
  ‚úì Owner: 123456789
```

> On first run with `.env` credentials, you'll also see a one-time migration line before the telegram line:
> `[config] Telegram credentials saved to ~/.safeclaw/telegram.json`

---

## Step 6 ‚Äî Connect to an LLM

Open Telegram, find your bot, and store your API key. **These commands work even while the gateway is dormant** ‚Äî you don't need to `/wake` first.

### Anthropic Claude

```
/auth anthropic sk-ant-api03-...
```

Default model: `claude-sonnet-4-5-20250929`

### OpenAI GPT

```
/auth openai sk-proj-...
```

Default model: `gpt-4o`

### Google Gemini (free tier available)

```
/auth gemini AIza...
```

Default model: `gemini-2.0-flash`. Get a free key at [aistudio.google.com](https://aistudio.google.com) ‚Äî no billing required.

### Ollama (local LLM ‚Äî no API key needed)

Ollama lets you run open-source LLMs entirely on your own machine. No cloud, no costs, full privacy.

#### Step A ‚Äî Install Ollama

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows ‚Äî download from https://ollama.com/download
```

#### Step B ‚Äî Pull a tool-capable model

```bash
ollama pull llama3.2          # recommended ‚Äî fast, supports tool calling
ollama pull qwen2.5           # strong alternative, good with structured output
ollama pull mistral-nemo      # good balance of speed and quality
```

> **Tool calling requirement:** SafeClaw needs models with native tool-calling support. Use `llama3.1`, `llama3.2`, `qwen2.5`, or `mistral-nemo`. SafeClaw uses Ollama's **native `/api/chat`** endpoint (not the OpenAI-compat layer), with schema normalisation to strip fields that confuse small models.

#### Step C ‚Äî Start Ollama

```bash
ollama serve
# Binds to http://localhost:11434 by default
```

#### Step D ‚Äî Register with SafeClaw

```
/auth ollama local
```

`local` is shorthand for `http://localhost:11434`. For a remote Ollama instance:

```
/auth ollama http://192.168.1.50:11434
```

#### Step E ‚Äî Select a model

```
/model ollama/llama3.2
```

### Check what's configured

```
/auth status
```

### Browse and switch models

```
/model                          ‚Üí list all models for every configured provider
/model list anthropic           ‚Üí list only Anthropic models
/model anthropic/claude-opus-4-6
/model openai/gpt-4o-mini
/model gemini/gemini-1.5-pro
```

Credentials are stored in `~/.safeclaw/auth.json`. They persist across restarts.

---

## Step 7 ‚Äî Wake Up and Go

```
/wake
```

On wake, SafeClaw:
1. Switches to `AWAKE` state ‚Äî all tools disabled by default
2. Loads your `soul.md` persona and prompt skills (background)
3. Probes system resources: CPU cores, RAM, GPU (nvidia-smi), Ollama models (background)
4. Connects to all configured MCP servers and discovers their tools (background)
5. Starts the 30-minute inactivity timer

```
/enable filesystem       ‚Üí allow file reads and writes
/enable browser          ‚Üí allow web browsing and URL auto-enrichment
/enable shell            ‚Üí allow shell commands (includes background processes)
/tools                   ‚Üí see everything and its ON/OFF status
/status                  ‚Üí gateway state + probed hardware info
```

Then talk naturally:

```
You:  what files are in my workspace?
Bot:  [lists files]

You:  write a Python script that fetches the Hacker News front page
Bot:  Action pending approval:
        Tool: filesystem/write_file
        Details: write_file: hn_fetch.py (312 chars)
        Expires in: 300s
      Reply /confirm a1b2c3d4 or /deny a1b2c3d4

You:  /confirm a1b2c3d4
Bot:  Approved. Done! I've written hn_fetch.py ...
```

---

## All Commands

### Lifecycle

| Command | Works dormant? | Description |
|---------|---------------|-------------|
| `/wake` | Yes | Wake the gateway |
| `/sleep` | No | Return to dormant, disconnect MCP servers, kill background processes |
| `/kill` | No | Emergency shutdown, stops the process |

### LLM Provider Setup

| Command | Works dormant? | Description |
|---------|---------------|-------------|
| `/auth <provider> <key>` | Yes | Store API key (`anthropic`, `openai`, `gemini`) or Ollama URL (`/auth ollama local`) |
| `/auth status` | Yes | Show all configured providers and the active one |
| `/auth remove <provider>` | Yes | Delete a stored API key |
| `/model` | Yes | List all available models fetched live from provider APIs |
| `/model list <provider>` | Yes | List models for one specific provider |
| `/model <provider/model>` | Yes | Switch to a specific model |

### Tools

| Command | Description |
|---------|-------------|
| `/tools` | List all tools (builtin + MCP + dynamic skills) with ON/OFF status |
| `/enable <tool>` | Enable a builtin tool (`filesystem`, `browser`, `shell`, `patch`, `memory`) |
| `/disable <tool>` | Disable a builtin tool |
| `/enable mcp:<server>` | Enable all tools for an MCP server |
| `/disable mcp:<server>` | Disable all tools for an MCP server |
| `/enable skill__<name>` | Enable a dynamically installed skill |
| `/disable skill__<name>` | Disable a dynamically installed skill |
| `/skills` | List prompt skills from `~/.safeclaw/prompt-skills/` |

### Permissions

| Command | Description |
|---------|-------------|
| `/confirm <id>` | Approve a pending dangerous action |
| `/deny <id>` | Reject a pending action |
| `/confirm` | List all pending approvals with their IDs |

### Info

| Command | Description |
|---------|-------------|
| `/status` | Gateway state, uptime, idle time, enabled tools, hardware info |
| `/audit [n]` | Last N audit log events (default 10) |
| `/audit verbose` | Toggle verbose mode (live üí≠ thinking + üîß tool messages during agent runs) |
| `/audit verbose on` | Enable verbose mode |
| `/audit verbose off` | Disable verbose mode |
| `/skills` | Prompt skills status and active count |
| `/help` | All commands inline in Telegram |

---

## Security Model

### Core guarantees

| Guarantee | How |
|-----------|-----|
| **Sleep-by-default** | Gateway starts dormant, ignores all messages except `/wake` from owner |
| **Single owner** | Only your Telegram user ID is authorised. Everyone else gets zero response |
| **Tools off by default** | All tools ‚Äî builtin and MCP ‚Äî are disabled on every wake |
| **Confirm before dangerous action** | Write, delete, execute, send, and background-spawn operations require `/confirm` |
| **Auto-sleep** | Inactivity timeout (default 30 min) returns to dormant; kills background processes |
| **Full audit trail** | Every event logged to `~/.safeclaw/audit.jsonl` |
| **Separate identity** | The bot is its own Telegram account, never acts as you |
| **Workspace sandboxing** | Filesystem tool restricted to `WORKSPACE_DIR` ‚Äî no `../` escape |
| **MCP isolation** | Each MCP server runs as a subprocess; crashes don't affect SafeClaw |

### SecretGuard ‚Äî LLM cannot see your secrets

SafeClaw has a dedicated security layer (`src/security/secret-guard.ts`) that sits between the LLM agent and the filesystem/shell tools. The LLM **cannot** access:

- Any `.env` or `.env.*` file
- `~/.safeclaw/auth.json` and `~/.safeclaw/*.json` (API keys and config)
- Any file whose name contains: `secret`, `password`, `credential`, `token` (case-insensitive)

If the LLM calls `read_file` on a protected path, it receives:
```
Access denied: this path is protected by SafeClaw security policy.
```

Shell output is additionally scrubbed: lines matching `KEY=...`, `TOKEN=...`, `SECRET=...`, `PASSWORD=...` have their values replaced with `[REDACTED]`. Shell commands that attempt to `cat` a protected file (e.g. `cat .env`, `cat ~/.safeclaw/auth.json`) are blocked before execution.

### Skill review

Dynamically proposed skills go through a two-stage process:
1. A dedicated **SkillCreator** sub-agent writes the code (not the main conversation LLM)
2. A **security Reviewer** agent checks the code for credential exposure, arbitrary execution, network exfiltration, and filesystem escape ‚Äî up to 2 revision attempts
3. The final code is always shown in full before you `/confirm`

---

## Infrastructure Awareness

On every `/wake`, SafeClaw probes your system resources in the background:

```
/status
State: AWAKE
CPU: 8 cores
RAM: 6.2/15.8 GB free
GPU: NVIDIA GeForce RTX 3060 (8.5 GB VRAM free)
Ollama: llama3.2 (2.0GB), qwen2.5 (4.7GB)
```

This information is injected into the **manager agent's** system prompt so it knows how many parallel workers to spawn:

| Free RAM | Parallel workers |
|----------|-----------------|
| < 4 GB (no GPU) | 1 |
| 4‚Äì8 GB (no GPU) | 2 |
| > 8 GB or GPU present | 4 |

The probe also selects the largest Ollama model that fits in available VRAM/RAM as the `recommendedModel` for the orchestrator.

---

## Multi-Agent Orchestration

For complex multi-step tasks, SafeClaw routes to an adaptive multi-agent pipeline instead of a single LLM call.

### Routing heuristic

Free-text messages are classified as "complex" if they:
- Contain 3 or more sentences, **or**
- Contain keywords like `build`, `create`, `generate`, `analyse`, `debug and fix`, `implement`, `design`

Simple messages (single questions, short commands) go directly to the single-agent path.

### Pipeline

```
User message
     ‚îÇ
     ‚ñº
 Complexity check
     ‚îÇ
     ‚îú‚îÄ Simple ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Single Agent (runAgent)
     ‚îÇ
     ‚îî‚îÄ Complex ‚îÄ‚ñ∫ Manager Agent (LLM)
                        ‚îÇ
                        ‚ñº
                   TaskPlan
                   { strategy: "parallel" | "sequential" | "direct",
                     subtasks: [...],
                     needsReview: boolean }
                        ‚îÇ
                        ‚îú‚îÄ direct ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Single Agent (runAgent)
                        ‚îÇ
                        ‚îú‚îÄ parallel ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Promise.all(workers) [capped by maxParallelWorkers]
                        ‚îÇ
                        ‚îî‚îÄ sequential ‚îÄ‚îÄ‚ñ∫ worker‚ÇÅ ‚Üí result‚ÇÅ ‚Üí worker‚ÇÇ(result‚ÇÅ) ‚Üí ...
                                                ‚îÇ
                                                ‚ñº
                                     Optional Reviewer Agent
                                     (validates output quality)
                                                ‚îÇ
                                                ‚ñº
                                     Assembled response ‚Üí user
```

### Agent roles

| Role | Description | Tool access |
|------|-------------|-------------|
| `manager` | Decomposes task into subtasks. Outputs JSON `TaskPlan`. Knows hardware limits. | None |
| `worker` | Executes one specific subtask. Runs safe actions immediately. | Full (read-only for dangerous ops) |
| `reviewer` | Validates worker output against original task. Outputs `{approved, feedback}`. | None |
| `skill_creator` | Writes skill code. Has access to filesystem read tools. | Read + filesystem write |

Workers execute **safe actions immediately** (read, list, browse) and report dangerous actions they would need as a list for the user to confirm. This keeps the approval model consistent even in multi-agent mode.

### Example

```
You:  build me a todo app with a REST API and tests

Bot:  üìã Task decomposed into 3 subtasks [sequential]:
        1. Design the data model and API endpoints
        2. Implement the Express.js server with all endpoints
        3. Write Jest tests for each endpoint

      [result from all workers assembled...]

      ‚ö†Ô∏è The following actions require /confirm before they can execute:
        ‚Ä¢ [Would execute] write_file: todo-app/server.js (842 chars)
        ‚Ä¢ [Would execute] write_file: todo-app/tests/api.test.js (512 chars)
```

---

## Background Process Execution

When the `shell` tool is enabled, the LLM can run commands in the background ‚Äî useful for long-running tasks like builds, installs, or servers.

```
You:  run npm install in the background

Bot:  Action pending approval: exec_shell_bg: npm install
      /confirm a1b2c3d4

You:  /confirm a1b2c3d4
Bot:  Background process started.
      Session ID: f3a9b2c1

You:  check on the npm install
Bot:  [calls process_poll ‚Äî returns accumulated output]
      added 842 packages in 23s
```

| Action | Safe? | Description |
|--------|-------|-------------|
| `exec_shell_bg` | Requires `/confirm` | Spawn a command, return session ID immediately |
| `process_poll` | Safe ‚Äî no confirm | Read accumulated output from a session |
| `process_list` | Safe ‚Äî no confirm | List all active/recent background sessions |
| `process_write` | Requires `/confirm` | Write to stdin of a running process |
| `process_kill` | Requires `/confirm` | Send SIGTERM to a running process |

Sessions are automatically cleaned up 30 minutes after the process exits. All running processes are terminated on `/sleep`, `/kill`, or auto-sleep.

---

## Self-Extending Skills

SafeClaw can detect when it lacks a capability and propose new skills at runtime ‚Äî without a restart.

### How it works (new flow)

1. The main LLM hits a capability gap ‚Üí calls `request_capability`
2. A dedicated **SkillCreator** sub-agent writes the complete skill code (not the main LLM)
3. A **security Reviewer** agent checks the code for vulnerabilities (up to 2 revision attempts)
4. The final code (with reviewer verdict) is sent to you as a proposal
5. `/confirm` installs it to `~/.safeclaw/skills/<name>.mjs` and activates it immediately

```
You:  create a PDF summary of my notes

Bot:  üîß Skill Proposal: pdf_create
      ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
      Description: Create PDF documents from text content
      Needed for: Generating a PDF summary of workspace notes

      ‚ö†Ô∏è  This skill performs potentially dangerous operations.
      ‚úÖ Security reviewer approved this code.

      Generated code:
      ```
      export const skill = { ... };
      ```

      ‚ö†Ô∏è  This code runs inside SafeClaw with full Node.js access.
      Review it carefully before approving.

      /confirm a1b2c3d4  ‚Üí  install skill
      /deny a1b2c3d4     ‚Üí  reject proposal
```

---

## MCP Tool Auto-Discovery

SafeClaw reads your Claude Code MCP settings and automatically discovers all tools on every `/wake`.

1. `/wake` is sent ‚Äî bot replies immediately (MCP discovery is non-blocking)
2. Background: reads `~/.claude/settings.json` ‚Üí `mcpServers`
3. For each `stdio` server: spawns process, calls `listTools()`, registers definitions
4. Tools appear in `/tools` grouped by server
5. Dangerous/safe classification by keyword heuristics (read/get/list/search ‚Üí safe; write/delete/create/send ‚Üí dangerous)

```
/enable mcp:github      ‚Üí enable all tools for the "github" MCP server
/disable mcp:github     ‚Üí disable them
/tools                  ‚Üí see full list including MCP tools
```

Example `~/.claude/settings.json`:

```json
{
  "mcpServers": {
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": { "GITHUB_TOKEN": "${GITHUB_TOKEN}" }
    }
  }
}
```

> **Note:** Only `stdio` servers (with a `command` field) are supported. HTTP/SSE servers are skipped.

---

## Customisation

### Soul File ‚Äî Custom Persona

Create `~/.safeclaw/soul.md` to override the default persona. Loaded on every `/wake` and appended to the system prompt (highest priority ‚Äî overrides defaults).

```markdown
# My Assistant

You are an expert DevOps assistant. Keep responses extremely terse.
Always suggest the simplest possible solution.
Prefer shell one-liners over multi-step processes.
```

### Prompt Skills ‚Äî Teach the LLM CLI Patterns

Drop `.md` files into `~/.safeclaw/prompt-skills/` to teach SafeClaw how to use specific CLI tools. SafeClaw checks whether required binaries are on your PATH and injects only active skills into the system prompt.

```markdown
---
title: GitHub CLI
bins: [gh]
---

## Using the GitHub CLI

Always prefer `gh` for GitHub operations:

- List open PRs: `gh pr list`
- View failed run logs: `gh run view --log-failed`
```

This skill activates only if `gh` is on PATH. Run `/skills` to see which are active.

**Frontmatter options:**

```yaml
---
title: My Tool         # shown in /skills ‚Äî defaults to filename
bins: [git, curl]      # ALL must be on PATH for skill to activate
anyBins: [jq, python3] # AT LEAST ONE must be on PATH
---
```

### Persistent Memory

The `memory` tool lets the agent remember facts across sessions:

```
You:  remember that my main project is at ~/projects/myapp
Bot:  [calls memory_write: "main_project_path" = "~/projects/myapp"]
      Stored.
```

Memory is stored in `~/.safeclaw/memories/` and automatically injected into every system prompt.

---

## Context Management

### Context window guard
Tool results larger than 8 KB are truncated before being added to conversation history. This prevents a single large file read from consuming the entire context window.

### Auto-compaction
When conversation history exceeds ~60,000 tokens, SafeClaw calls the LLM to summarise the oldest 20 messages into a compact block. You'll see: `üì¶ Conversation compacted to fit context window.`

### Message debouncing
Multiple messages sent within 500 ms are merged into a single agent run. Prevents duplicate LLM calls from burst typing.

### URL auto-enrichment
When `browser` is enabled and your message contains a URL, SafeClaw fetches it silently and prepends the content to the message ‚Äî the LLM sees the page without needing a tool call. Up to 3 URLs per message, capped at 6 KB each.

---

## Project Structure

```
safeclaw/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts                  # Entry point and startup banner
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.ts              # All TypeScript interfaces/enums incl. InfraContext
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gateway.ts            # State machine (dormant/awake/action_pending/shutdown)
‚îÇ   ‚îÇ   ‚îÇ                         #   probes infra + connects MCP on wake
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.ts               # Single-owner Telegram ID check
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.ts             # Loads credentials from ~/.safeclaw/telegram.json (primary)
‚îÇ                             #   or .env (first-run fallback, auto-migrates)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ channels/telegram/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts             # grammy bot setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handler.ts            # Inbound routing, auth check, 500ms debounce
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sender.ts             # Outbound with chunking for long replies
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ free-text.ts          # URL enrichment + routes to runAgent/runOrchestrated
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.ts              # LLMProvider interface, ProviderName, model defaults
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic.ts          # Anthropic Claude client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.ts             # OpenAI client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini.ts             # Google Gemini client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama.ts             # Ollama native /api/chat client + schema normaliser
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.ts             # Live model listing from provider APIs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store.ts              # Persists API keys to ~/.safeclaw/auth.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resolver.ts           # Picks the active provider + model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retry.ts              # Retry wrapper for transient API errors
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ session.ts            # Conversation history + orphan repair + token estimate
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tool-schemas.ts       # ToolDefinition ‚Üí LLM tool_use schemas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ runner.ts             # Main LLM loop: system prompt, safe execute,
‚îÇ   ‚îÇ                             #   dangerous queue, context guard, auto-compaction,
‚îÇ   ‚îÇ                             #   SkillCreator delegation on request_capability
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/                   # Multi-agent orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ roles.ts              # Role system prompts (manager/worker/reviewer/skill_creator)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sub-agent.ts          # Ephemeral SubAgent: runs tool loop, safe actions only
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.ts       # runOrchestrated: manager‚Üíworkers‚Üíreviewer pipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ skill-creator.ts      # createSkillWithReview: SkillCreator + security Reviewer
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry.ts           # Tool map: enable/disable, MCP register/clear
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executor.ts           # Dispatches to real impl, MCP callTool, skill call
‚îÇ   ‚îÇ   ‚îÇ                         #   SecretGuard checks before every filesystem op
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ filesystem.ts         # Real fs: read, list, write, delete, move (sandboxed)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ browser.ts            # Real: fetch + Readability extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shell.ts              # Real: child_process exec with 30s timeout
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patch.ts              # Real: apply Add/Update/Delete/Move patches
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory.ts             # Persistent key-value memory store
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ process-registry.ts   # Background process sessions + TTL sweeper
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ security/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ secret-guard.ts       # SecretGuard: blocks protected paths, redacts env vars,
‚îÇ   ‚îÇ                             #   checks shell commands for secret reads
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ probe.ts              # probeInfra(): CPU/RAM/GPU/Ollama models
‚îÇ   ‚îÇ                             #   getResourceLimits(): maxWorkers, recommendedModel
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ skills/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamic.ts            # DynamicSkill interface + .mjs file loader
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manager.ts            # SkillsManager: install, load, list, persist
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompt-skills.ts      # SKILL.md loader with bin-check + prompt injection
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ mcp/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.ts             # Reads ~/.claude/settings.json mcpServers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manager.ts            # Connect/discover/call/disconnect MCP servers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts              # Barrel export
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ permissions/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ store.ts              # Pending approval store with 5-min expiry
‚îÇ   ‚îú‚îÄ‚îÄ audit/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.ts             # Append-only JSONL event logger
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.ts             # /command tokenizer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handlers.ts           # Handler for each command
‚îÇ   ‚îî‚îÄ‚îÄ storage/
‚îÇ       ‚îî‚îÄ‚îÄ persistence.ts        # JSON/JSONL read-write helpers
‚îÇ
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ CLAUDE.md                     # Architecture and developer reference
```

### User data directories (`~/.safeclaw/`)

```
~/.safeclaw/
‚îú‚îÄ‚îÄ telegram.json                 # Bot token + owner ID (created on first run ‚Äî keep private)
‚îú‚îÄ‚îÄ auth.json                     # LLM API keys for Anthropic/OpenAI/Gemini/Ollama
‚îú‚îÄ‚îÄ audit.jsonl                   # Append-only audit log
‚îú‚îÄ‚îÄ soul.md                       # Optional custom persona (injected on wake)
‚îú‚îÄ‚îÄ memories/                     # Persistent agent memory (key-value store)
‚îú‚îÄ‚îÄ prompt-skills/                # SKILL.md files ‚Äî teach LLM CLI patterns
‚îÇ   ‚îú‚îÄ‚îÄ weather.md
‚îÇ   ‚îú‚îÄ‚îÄ github.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ skills/                       # Dynamically installed JS skills
    ‚îú‚îÄ‚îÄ pdf_create.mjs
    ‚îî‚îÄ‚îÄ ...
```

> All `*.json` files in `~/.safeclaw/` are blocked by SecretGuard ‚Äî the LLM cannot read `telegram.json` or `auth.json` even if asked.

---

## What's Implemented

| Feature | Status |
|---------|--------|
| Gateway state machine (dormant/awake/action_pending/shutdown) | ‚úÖ |
| Single-owner auth with silent drop | ‚úÖ |
| Runtime tool enable/disable | ‚úÖ |
| `/confirm` dangerous action flow with 5-min expiry | ‚úÖ |
| JSONL audit log | ‚úÖ |
| Telegram bot (grammy) | ‚úÖ |
| LLM agent ‚Äî Anthropic, OpenAI, Gemini, Ollama | ‚úÖ |
| Ollama native `/api/chat` with schema normalisation | ‚úÖ |
| Live model listing from provider APIs | ‚úÖ |
| Persistent API key storage | ‚úÖ |
| Real filesystem tool (sandboxed to `WORKSPACE_DIR`) | ‚úÖ |
| Real browser tool (fetch + Readability extraction) | ‚úÖ |
| Real shell tool (`child_process`, 30s timeout) | ‚úÖ |
| Apply-patch tool (Add/Update/Delete/Move) | ‚úÖ |
| MCP auto-discovery from `~/.claude/settings.json` | ‚úÖ |
| Self-extending dynamic skills (SkillCreator + security review) | ‚úÖ |
| Soul file ‚Äî custom persona | ‚úÖ |
| Prompt skills ‚Äî SKILL.md files injected into system prompt | ‚úÖ |
| URL auto-enrichment ‚Äî auto-fetch URLs in messages | ‚úÖ |
| Message debouncing ‚Äî merge burst messages | ‚úÖ |
| Context window guard ‚Äî truncate large tool results | ‚úÖ |
| Auto-compaction ‚Äî LLM summarises old history | ‚úÖ |
| Background process execution ‚Äî `exec_shell_bg` + poll/write/kill | ‚úÖ |
| Persistent memory across sessions | ‚úÖ |
| **SecretGuard ‚Äî LLM blocked from reading secrets** | ‚úÖ |
| **Infrastructure probe ‚Äî CPU/RAM/GPU/Ollama on wake** | ‚úÖ |
| **Multi-agent orchestration ‚Äî manager/worker/reviewer pipeline** | ‚úÖ |
| **SkillCreator agent ‚Äî dedicated skill writing + security review** | ‚úÖ |
| **Verbose audit ‚Äî live üí≠/üîß/‚úÖ messages during agent runs** | ‚úÖ |
| **Telegram credentials in `~/.safeclaw/telegram.json` (out of project dir)** | ‚úÖ |

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Runtime | Node.js 22+ |
| Language | TypeScript 5 (strict mode) |
| Telegram | [grammy](https://grammy.dev) |
| LLM: Anthropic | Anthropic Messages API (raw fetch) |
| LLM: OpenAI | OpenAI Chat Completions API (raw fetch) |
| LLM: Gemini | Google Generative Language API (raw fetch) |
| LLM: Ollama | Ollama native `/api/chat` (raw fetch, no wrapper library) |
| Browser | `@mozilla/readability` + `linkedom` |
| MCP | `@modelcontextprotocol/sdk` ^1.12.0 |
| Storage | File-based JSON + JSONL (no database) |
| Dev runner | `tsx` (TypeScript execute, no build step needed) |

---

## Roadmap

- [x] Gateway state machine, Telegram integration, commands, audit
- [x] Real filesystem tools with path sandboxing
- [x] LLM agent ‚Äî Anthropic, OpenAI, Gemini, Ollama ‚Äî with tool calling
- [x] Ollama native API + schema normalisation for small models
- [x] Live model listing from provider APIs
- [x] MCP tool auto-discovery from `~/.claude/settings.json`
- [x] Self-extending skills ‚Äî SkillCreator agent + security Reviewer
- [x] Real browser tool (fetch + Readability)
- [x] Real shell execution (with timeout and output limits)
- [x] Apply-patch tool for code editing
- [x] Soul file ‚Äî custom persona without code changes
- [x] Prompt skills ‚Äî SKILL.md files teach CLI patterns
- [x] URL auto-enrichment in messages
- [x] Message debouncing
- [x] Context window guard
- [x] Auto-compaction of conversation history
- [x] Background process execution with poll/write/kill
- [x] Persistent memory across sessions
- [x] SecretGuard ‚Äî LLM can never read API keys, `.env` files, or `telegram.json`
- [x] Telegram credentials stored in `~/.safeclaw/telegram.json` (auto-migrated from `.env`)
- [x] Verbose audit mode ‚Äî `/audit verbose` streams live LLM thinking + tool events
- [x] Infrastructure probe ‚Äî hardware-aware orchestration
- [x] Multi-agent orchestration ‚Äî manager/worker/reviewer pipeline
- [ ] HTTP/SSE MCP server support
- [ ] WhatsApp Cloud API integration
- [ ] Web dashboard for visual tool management
- [ ] Multi-owner support with invite codes
- [ ] Semantic memory (SQLite + vector search across sessions)

---

## Contributing

Contributions welcome! Please open an issue first to discuss what you'd like to change. The architecture is documented in `CLAUDE.md`.

## License

[MIT](LICENSE)
