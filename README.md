# SafeClaw â€” Secure Personal AI Assistant

**Sleep-by-default. Tools off by default. You hold the keys.**

SafeClaw is a privacy-first AI assistant you control from your phone via Telegram. You connect it to your own LLM API key (Anthropic Claude, OpenAI GPT, Google Gemini, or a local Ollama instance). It auto-discovers tools from any MCP servers you've configured, adapts to your available hardware, and orchestrates multi-step tasks using a managerâ€“workerâ€“reviewer agent pipeline â€” all without any telemetry, cloud accounts, or third-party data handling.

Unlike always-on AI gateways, SafeClaw inverts the defaults:

| Problem with most AI gateways | SafeClaw's answer |
|-------------------------------|-------------------|
| Bot hijacks your messaging account | **Own identity** â€” SafeClaw is its own Telegram bot |
| Always-on with a large attack surface | **Dormant by default** â€” only wakes on `/wake` |
| Static tool permissions set in config | **Runtime toggle** â€” `/enable browser`, `/disable shell`, on the fly |
| Dangerous actions execute immediately | **Explicit approval** â€” every write, delete, execute requires `/confirm` |
| Unknown senders get error responses | **Silent drop** â€” non-owners receive zero response, zero acknowledgment |
| LLM can read your API keys and `.env` | **SecretGuard** â€” protected paths blocked at the tool layer, never reach the LLM |
| Agent ignores hardware constraints | **Infra-aware** â€” probes CPU/RAM/GPU/Ollama on wake, calibrates worker count |
| One LLM handles everything linearly | **Multi-agent** â€” manager decomposes complex tasks into parallel/sequential workers |

---

## Prerequisites

- **Node.js 22+** (`node --version` to check)
- A **Telegram account** (to talk to the bot)
- At least one of:
  - **Anthropic** API key â€” [console.anthropic.com](https://console.anthropic.com) â†’ API Keys
  - **OpenAI** API key â€” [platform.openai.com](https://platform.openai.com) â†’ API Keys
  - **Google Gemini** API key â€” [aistudio.google.com](https://aistudio.google.com) â†’ Get API Key *(free tier available)*
  - **Ollama** â€” free, runs entirely on your machine, no API key required (see below)

---

## Step 1 â€” Create a Telegram Bot

1. Open Telegram and message **[@BotFather](https://t.me/BotFather)**
2. Send `/newbot` and follow the prompts
3. Copy the **bot token** â€” looks like `7412345678:AAFz...`

> Keep this token private. Anyone with it can control your bot.

---

## Step 2 â€” Find Your Telegram User ID

1. Message **[@userinfobot](https://t.me/userinfobot)** on Telegram
2. It replies with your numeric user ID â€” looks like `123456789`

This ID is how SafeClaw knows you're the owner. Every message from any other ID is silently dropped.

---

## Step 3 â€” Install SafeClaw

```bash
git clone https://github.com/yourname/safeclaw.git
cd safeclaw
npm install
```

---

## Step 4 â€” Configure

```bash
cp .env.example .env
```

Edit `.env`:

```env
TELEGRAM_BOT_TOKEN=7412345678:AAFz...       # from BotFather
OWNER_TELEGRAM_ID=123456789                 # your numeric Telegram ID
INACTIVITY_TIMEOUT_MINUTES=30               # optional, default 30
WORKSPACE_DIR=/home/you/safeclaw-workspace  # optional, default ~/safeclaw-workspace
```

> `WORKSPACE_DIR` is the only directory SafeClaw's filesystem tool can read or write. Paths that try to escape it (e.g. `../../etc/passwd`) are rejected. `.env` files and `~/.safeclaw/*.json` are additionally blocked by SecretGuard even inside the workspace.

---

## Step 5 â€” Run

```bash
# Development (auto-reloads on file changes)
npm run dev

# Production
npm start
```

You should see:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SafeClaw â€” Secure AI Assistant     â”‚
â”‚   Sleep-by-default. You hold the keysâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[telegram] Bot online: @YourBotName
[telegram] Send /wake from your Telegram to activate
Security status:
  âœ“ Gateway: DORMANT (ignoring all messages except /wake)
  âœ“ Tools: ALL DISABLED
  âœ“ Authentication: single-owner (Telegram ID)
  âœ“ Owner: 123456789
```

---

## Step 6 â€” Connect to an LLM

Open Telegram, find your bot, and store your API key. **These commands work even while the gateway is dormant** â€” you don't need to `/wake` first.

### Anthropic Claude

```
/auth anthropic sk-ant-api03-...
```

Default model: `claude-sonnet-4-5-20250929`

### OpenAI GPT

```
/auth openai sk-proj-...
```

Default model: `gpt-4o`

### Google Gemini (free tier available)

```
/auth gemini AIza...
```

Default model: `gemini-2.0-flash`. Get a free key at [aistudio.google.com](https://aistudio.google.com) â€” no billing required.

### Ollama (local LLM â€” no API key needed)

Ollama lets you run open-source LLMs entirely on your own machine. No cloud, no costs, full privacy.

#### Step A â€” Install Ollama

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows â€” download from https://ollama.com/download
```

#### Step B â€” Pull a tool-capable model

```bash
ollama pull llama3.2          # recommended â€” fast, supports tool calling
ollama pull qwen2.5           # strong alternative, good with structured output
ollama pull mistral-nemo      # good balance of speed and quality
```

> **Tool calling requirement:** SafeClaw needs models with native tool-calling support. Use `llama3.1`, `llama3.2`, `qwen2.5`, or `mistral-nemo`. SafeClaw uses Ollama's **native `/api/chat`** endpoint (not the OpenAI-compat layer), with schema normalisation to strip fields that confuse small models.

#### Step C â€” Start Ollama

```bash
ollama serve
# Binds to http://localhost:11434 by default
```

#### Step D â€” Register with SafeClaw

```
/auth ollama local
```

`local` is shorthand for `http://localhost:11434`. For a remote Ollama instance:

```
/auth ollama http://192.168.1.50:11434
```

#### Step E â€” Select a model

```
/model ollama/llama3.2
```

### Check what's configured

```
/auth status
```

### Browse and switch models

```
/model                          â†’ list all models for every configured provider
/model list anthropic           â†’ list only Anthropic models
/model anthropic/claude-opus-4-6
/model openai/gpt-4o-mini
/model gemini/gemini-1.5-pro
```

Credentials are stored in `~/.safeclaw/auth.json`. They persist across restarts.

---

## Step 7 â€” Wake Up and Go

```
/wake
```

On wake, SafeClaw:
1. Switches to `AWAKE` state â€” all tools disabled by default
2. Loads your `soul.md` persona and prompt skills (background)
3. Probes system resources: CPU cores, RAM, GPU (nvidia-smi), Ollama models (background)
4. Connects to all configured MCP servers and discovers their tools (background)
5. Starts the 30-minute inactivity timer

```
/enable filesystem       â†’ allow file reads and writes
/enable browser          â†’ allow web browsing and URL auto-enrichment
/enable shell            â†’ allow shell commands (includes background processes)
/tools                   â†’ see everything and its ON/OFF status
/status                  â†’ gateway state + probed hardware info
```

Then talk naturally:

```
You:  what files are in my workspace?
Bot:  [lists files]

You:  write a Python script that fetches the Hacker News front page
Bot:  Action pending approval:
        Tool: filesystem/write_file
        Details: write_file: hn_fetch.py (312 chars)
        Expires in: 300s
      Reply /confirm a1b2c3d4 or /deny a1b2c3d4

You:  /confirm a1b2c3d4
Bot:  Approved. Done! I've written hn_fetch.py ...
```

---

## All Commands

### Lifecycle

| Command | Works dormant? | Description |
|---------|---------------|-------------|
| `/wake` | Yes | Wake the gateway |
| `/sleep` | No | Return to dormant, disconnect MCP servers, kill background processes |
| `/kill` | No | Emergency shutdown, stops the process |

### LLM Provider Setup

| Command | Works dormant? | Description |
|---------|---------------|-------------|
| `/auth <provider> <key>` | Yes | Store API key (`anthropic`, `openai`, `gemini`) or Ollama URL (`/auth ollama local`) |
| `/auth status` | Yes | Show all configured providers and the active one |
| `/auth remove <provider>` | Yes | Delete a stored API key |
| `/model` | Yes | List all available models fetched live from provider APIs |
| `/model list <provider>` | Yes | List models for one specific provider |
| `/model <provider/model>` | Yes | Switch to a specific model |

### Tools

| Command | Description |
|---------|-------------|
| `/tools` | List all tools (builtin + MCP + dynamic skills) with ON/OFF status |
| `/enable <tool>` | Enable a builtin tool (`filesystem`, `browser`, `shell`, `patch`, `memory`) |
| `/disable <tool>` | Disable a builtin tool |
| `/enable mcp:<server>` | Enable all tools for an MCP server |
| `/disable mcp:<server>` | Disable all tools for an MCP server |
| `/enable skill__<name>` | Enable a dynamically installed skill |
| `/disable skill__<name>` | Disable a dynamically installed skill |
| `/skills` | List prompt skills from `~/.safeclaw/prompt-skills/` |

### Permissions

| Command | Description |
|---------|-------------|
| `/confirm <id>` | Approve a pending dangerous action |
| `/deny <id>` | Reject a pending action |
| `/confirm` | List all pending approvals with their IDs |

### Info

| Command | Description |
|---------|-------------|
| `/status` | Gateway state, uptime, idle time, enabled tools, hardware info |
| `/audit [n]` | Last N audit log events (default 10) |
| `/skills` | Prompt skills status and active count |
| `/help` | All commands inline in Telegram |

---

## Security Model

### Core guarantees

| Guarantee | How |
|-----------|-----|
| **Sleep-by-default** | Gateway starts dormant, ignores all messages except `/wake` from owner |
| **Single owner** | Only your Telegram user ID is authorised. Everyone else gets zero response |
| **Tools off by default** | All tools â€” builtin and MCP â€” are disabled on every wake |
| **Confirm before dangerous action** | Write, delete, execute, send, and background-spawn operations require `/confirm` |
| **Auto-sleep** | Inactivity timeout (default 30 min) returns to dormant; kills background processes |
| **Full audit trail** | Every event logged to `~/.safeclaw/audit.jsonl` |
| **Separate identity** | The bot is its own Telegram account, never acts as you |
| **Workspace sandboxing** | Filesystem tool restricted to `WORKSPACE_DIR` â€” no `../` escape |
| **MCP isolation** | Each MCP server runs as a subprocess; crashes don't affect SafeClaw |

### SecretGuard â€” LLM cannot see your secrets

SafeClaw has a dedicated security layer (`src/security/secret-guard.ts`) that sits between the LLM agent and the filesystem/shell tools. The LLM **cannot** access:

- Any `.env` or `.env.*` file
- `~/.safeclaw/auth.json` and `~/.safeclaw/*.json` (API keys and config)
- Any file whose name contains: `secret`, `password`, `credential`, `token` (case-insensitive)

If the LLM calls `read_file` on a protected path, it receives:
```
Access denied: this path is protected by SafeClaw security policy.
```

Shell output is additionally scrubbed: lines matching `KEY=...`, `TOKEN=...`, `SECRET=...`, `PASSWORD=...` have their values replaced with `[REDACTED]`. Shell commands that attempt to `cat` a protected file (e.g. `cat .env`, `cat ~/.safeclaw/auth.json`) are blocked before execution.

### Skill review

Dynamically proposed skills go through a two-stage process:
1. A dedicated **SkillCreator** sub-agent writes the code (not the main conversation LLM)
2. A **security Reviewer** agent checks the code for credential exposure, arbitrary execution, network exfiltration, and filesystem escape â€” up to 2 revision attempts
3. The final code is always shown in full before you `/confirm`

---

## Infrastructure Awareness

On every `/wake`, SafeClaw probes your system resources in the background:

```
/status
State: AWAKE
CPU: 8 cores
RAM: 6.2/15.8 GB free
GPU: NVIDIA GeForce RTX 3060 (8.5 GB VRAM free)
Ollama: llama3.2 (2.0GB), qwen2.5 (4.7GB)
```

This information is injected into the **manager agent's** system prompt so it knows how many parallel workers to spawn:

| Free RAM | Parallel workers |
|----------|-----------------|
| < 4 GB (no GPU) | 1 |
| 4â€“8 GB (no GPU) | 2 |
| > 8 GB or GPU present | 4 |

The probe also selects the largest Ollama model that fits in available VRAM/RAM as the `recommendedModel` for the orchestrator.

---

## Multi-Agent Orchestration

For complex multi-step tasks, SafeClaw routes to an adaptive multi-agent pipeline instead of a single LLM call.

### Routing heuristic

Free-text messages are classified as "complex" if they:
- Contain 3 or more sentences, **or**
- Contain keywords like `build`, `create`, `generate`, `analyse`, `debug and fix`, `implement`, `design`

Simple messages (single questions, short commands) go directly to the single-agent path.

### Pipeline

```
User message
     â”‚
     â–¼
 Complexity check
     â”‚
     â”œâ”€ Simple â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Single Agent (runAgent)
     â”‚
     â””â”€ Complex â”€â–º Manager Agent (LLM)
                        â”‚
                        â–¼
                   TaskPlan
                   { strategy: "parallel" | "sequential" | "direct",
                     subtasks: [...],
                     needsReview: boolean }
                        â”‚
                        â”œâ”€ direct â”€â”€â”€â”€â”€â”€â–º Single Agent (runAgent)
                        â”‚
                        â”œâ”€ parallel â”€â”€â”€â”€â–º Promise.all(workers) [capped by maxParallelWorkers]
                        â”‚
                        â””â”€ sequential â”€â”€â–º workerâ‚ â†’ resultâ‚ â†’ workerâ‚‚(resultâ‚) â†’ ...
                                                â”‚
                                                â–¼
                                     Optional Reviewer Agent
                                     (validates output quality)
                                                â”‚
                                                â–¼
                                     Assembled response â†’ user
```

### Agent roles

| Role | Description | Tool access |
|------|-------------|-------------|
| `manager` | Decomposes task into subtasks. Outputs JSON `TaskPlan`. Knows hardware limits. | None |
| `worker` | Executes one specific subtask. Runs safe actions immediately. | Full (read-only for dangerous ops) |
| `reviewer` | Validates worker output against original task. Outputs `{approved, feedback}`. | None |
| `skill_creator` | Writes skill code. Has access to filesystem read tools. | Read + filesystem write |

Workers execute **safe actions immediately** (read, list, browse) and report dangerous actions they would need as a list for the user to confirm. This keeps the approval model consistent even in multi-agent mode.

### Example

```
You:  build me a todo app with a REST API and tests

Bot:  ğŸ“‹ Task decomposed into 3 subtasks [sequential]:
        1. Design the data model and API endpoints
        2. Implement the Express.js server with all endpoints
        3. Write Jest tests for each endpoint

      [result from all workers assembled...]

      âš ï¸ The following actions require /confirm before they can execute:
        â€¢ [Would execute] write_file: todo-app/server.js (842 chars)
        â€¢ [Would execute] write_file: todo-app/tests/api.test.js (512 chars)
```

---

## Background Process Execution

When the `shell` tool is enabled, the LLM can run commands in the background â€” useful for long-running tasks like builds, installs, or servers.

```
You:  run npm install in the background

Bot:  Action pending approval: exec_shell_bg: npm install
      /confirm a1b2c3d4

You:  /confirm a1b2c3d4
Bot:  Background process started.
      Session ID: f3a9b2c1

You:  check on the npm install
Bot:  [calls process_poll â€” returns accumulated output]
      added 842 packages in 23s
```

| Action | Safe? | Description |
|--------|-------|-------------|
| `exec_shell_bg` | Requires `/confirm` | Spawn a command, return session ID immediately |
| `process_poll` | Safe â€” no confirm | Read accumulated output from a session |
| `process_list` | Safe â€” no confirm | List all active/recent background sessions |
| `process_write` | Requires `/confirm` | Write to stdin of a running process |
| `process_kill` | Requires `/confirm` | Send SIGTERM to a running process |

Sessions are automatically cleaned up 30 minutes after the process exits. All running processes are terminated on `/sleep`, `/kill`, or auto-sleep.

---

## Self-Extending Skills

SafeClaw can detect when it lacks a capability and propose new skills at runtime â€” without a restart.

### How it works (new flow)

1. The main LLM hits a capability gap â†’ calls `request_capability`
2. A dedicated **SkillCreator** sub-agent writes the complete skill code (not the main LLM)
3. A **security Reviewer** agent checks the code for vulnerabilities (up to 2 revision attempts)
4. The final code (with reviewer verdict) is sent to you as a proposal
5. `/confirm` installs it to `~/.safeclaw/skills/<name>.mjs` and activates it immediately

```
You:  create a PDF summary of my notes

Bot:  ğŸ”§ Skill Proposal: pdf_create
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      Description: Create PDF documents from text content
      Needed for: Generating a PDF summary of workspace notes

      âš ï¸  This skill performs potentially dangerous operations.
      âœ… Security reviewer approved this code.

      Generated code:
      ```
      export const skill = { ... };
      ```

      âš ï¸  This code runs inside SafeClaw with full Node.js access.
      Review it carefully before approving.

      /confirm a1b2c3d4  â†’  install skill
      /deny a1b2c3d4     â†’  reject proposal
```

---

## MCP Tool Auto-Discovery

SafeClaw reads your Claude Code MCP settings and automatically discovers all tools on every `/wake`.

1. `/wake` is sent â€” bot replies immediately (MCP discovery is non-blocking)
2. Background: reads `~/.claude/settings.json` â†’ `mcpServers`
3. For each `stdio` server: spawns process, calls `listTools()`, registers definitions
4. Tools appear in `/tools` grouped by server
5. Dangerous/safe classification by keyword heuristics (read/get/list/search â†’ safe; write/delete/create/send â†’ dangerous)

```
/enable mcp:github      â†’ enable all tools for the "github" MCP server
/disable mcp:github     â†’ disable them
/tools                  â†’ see full list including MCP tools
```

Example `~/.claude/settings.json`:

```json
{
  "mcpServers": {
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": { "GITHUB_TOKEN": "${GITHUB_TOKEN}" }
    }
  }
}
```

> **Note:** Only `stdio` servers (with a `command` field) are supported. HTTP/SSE servers are skipped.

---

## Customisation

### Soul File â€” Custom Persona

Create `~/.safeclaw/soul.md` to override the default persona. Loaded on every `/wake` and appended to the system prompt (highest priority â€” overrides defaults).

```markdown
# My Assistant

You are an expert DevOps assistant. Keep responses extremely terse.
Always suggest the simplest possible solution.
Prefer shell one-liners over multi-step processes.
```

### Prompt Skills â€” Teach the LLM CLI Patterns

Drop `.md` files into `~/.safeclaw/prompt-skills/` to teach SafeClaw how to use specific CLI tools. SafeClaw checks whether required binaries are on your PATH and injects only active skills into the system prompt.

```markdown
---
title: GitHub CLI
bins: [gh]
---

## Using the GitHub CLI

Always prefer `gh` for GitHub operations:

- List open PRs: `gh pr list`
- View failed run logs: `gh run view --log-failed`
```

This skill activates only if `gh` is on PATH. Run `/skills` to see which are active.

**Frontmatter options:**

```yaml
---
title: My Tool         # shown in /skills â€” defaults to filename
bins: [git, curl]      # ALL must be on PATH for skill to activate
anyBins: [jq, python3] # AT LEAST ONE must be on PATH
---
```

### Persistent Memory

The `memory` tool lets the agent remember facts across sessions:

```
You:  remember that my main project is at ~/projects/myapp
Bot:  [calls memory_write: "main_project_path" = "~/projects/myapp"]
      Stored.
```

Memory is stored in `~/.safeclaw/memories/` and automatically injected into every system prompt.

---

## Context Management

### Context window guard
Tool results larger than 8 KB are truncated before being added to conversation history. This prevents a single large file read from consuming the entire context window.

### Auto-compaction
When conversation history exceeds ~60,000 tokens, SafeClaw calls the LLM to summarise the oldest 20 messages into a compact block. You'll see: `ğŸ“¦ Conversation compacted to fit context window.`

### Message debouncing
Multiple messages sent within 500 ms are merged into a single agent run. Prevents duplicate LLM calls from burst typing.

### URL auto-enrichment
When `browser` is enabled and your message contains a URL, SafeClaw fetches it silently and prepends the content to the message â€” the LLM sees the page without needing a tool call. Up to 3 URLs per message, capped at 6 KB each.

---

## Project Structure

```
safeclaw/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts                  # Entry point and startup banner
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ types.ts              # All TypeScript interfaces/enums incl. InfraContext
â”‚   â”‚   â”œâ”€â”€ gateway.ts            # State machine (dormant/awake/action_pending/shutdown)
â”‚   â”‚   â”‚                         #   probes infra + connects MCP on wake
â”‚   â”‚   â”œâ”€â”€ auth.ts               # Single-owner Telegram ID check
â”‚   â”‚   â””â”€â”€ config.ts             # .env loader and config validation
â”‚   â”‚
â”‚   â”œâ”€â”€ channels/telegram/
â”‚   â”‚   â”œâ”€â”€ client.ts             # grammy bot setup
â”‚   â”‚   â”œâ”€â”€ handler.ts            # Inbound routing, auth check, 500ms debounce
â”‚   â”‚   â”œâ”€â”€ sender.ts             # Outbound with chunking for long replies
â”‚   â”‚   â””â”€â”€ free-text.ts          # URL enrichment + routes to runAgent/runOrchestrated
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ types.ts              # LLMProvider interface, ProviderName, model defaults
â”‚   â”‚   â”œâ”€â”€ anthropic.ts          # Anthropic Claude client
â”‚   â”‚   â”œâ”€â”€ openai.ts             # OpenAI client
â”‚   â”‚   â”œâ”€â”€ gemini.ts             # Google Gemini client
â”‚   â”‚   â”œâ”€â”€ ollama.ts             # Ollama native /api/chat client + schema normaliser
â”‚   â”‚   â”œâ”€â”€ models.ts             # Live model listing from provider APIs
â”‚   â”‚   â”œâ”€â”€ store.ts              # Persists API keys to ~/.safeclaw/auth.json
â”‚   â”‚   â”œâ”€â”€ resolver.ts           # Picks the active provider + model
â”‚   â”‚   â””â”€â”€ retry.ts              # Retry wrapper for transient API errors
â”‚   â”‚
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ session.ts            # Conversation history + orphan repair + token estimate
â”‚   â”‚   â”œâ”€â”€ tool-schemas.ts       # ToolDefinition â†’ LLM tool_use schemas
â”‚   â”‚   â””â”€â”€ runner.ts             # Main LLM loop: system prompt, safe execute,
â”‚   â”‚                             #   dangerous queue, context guard, auto-compaction,
â”‚   â”‚                             #   SkillCreator delegation on request_capability
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                   # Multi-agent orchestration
â”‚   â”‚   â”œâ”€â”€ roles.ts              # Role system prompts (manager/worker/reviewer/skill_creator)
â”‚   â”‚   â”œâ”€â”€ sub-agent.ts          # Ephemeral SubAgent: runs tool loop, safe actions only
â”‚   â”‚   â”œâ”€â”€ orchestrator.ts       # runOrchestrated: managerâ†’workersâ†’reviewer pipeline
â”‚   â”‚   â””â”€â”€ skill-creator.ts      # createSkillWithReview: SkillCreator + security Reviewer
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ registry.ts           # Tool map: enable/disable, MCP register/clear
â”‚   â”‚   â”œâ”€â”€ executor.ts           # Dispatches to real impl, MCP callTool, skill call
â”‚   â”‚   â”‚                         #   SecretGuard checks before every filesystem op
â”‚   â”‚   â”œâ”€â”€ filesystem.ts         # Real fs: read, list, write, delete, move (sandboxed)
â”‚   â”‚   â”œâ”€â”€ browser.ts            # Real: fetch + Readability extraction
â”‚   â”‚   â”œâ”€â”€ shell.ts              # Real: child_process exec with 30s timeout
â”‚   â”‚   â”œâ”€â”€ patch.ts              # Real: apply Add/Update/Delete/Move patches
â”‚   â”‚   â”œâ”€â”€ memory.ts             # Persistent key-value memory store
â”‚   â”‚   â””â”€â”€ process-registry.ts   # Background process sessions + TTL sweeper
â”‚   â”‚
â”‚   â”œâ”€â”€ security/
â”‚   â”‚   â””â”€â”€ secret-guard.ts       # SecretGuard: blocks protected paths, redacts env vars,
â”‚   â”‚                             #   checks shell commands for secret reads
â”‚   â”‚
â”‚   â”œâ”€â”€ infra/
â”‚   â”‚   â””â”€â”€ probe.ts              # probeInfra(): CPU/RAM/GPU/Ollama models
â”‚   â”‚                             #   getResourceLimits(): maxWorkers, recommendedModel
â”‚   â”‚
â”‚   â”œâ”€â”€ skills/
â”‚   â”‚   â”œâ”€â”€ dynamic.ts            # DynamicSkill interface + .mjs file loader
â”‚   â”‚   â”œâ”€â”€ manager.ts            # SkillsManager: install, load, list, persist
â”‚   â”‚   â””â”€â”€ prompt-skills.ts      # SKILL.md loader with bin-check + prompt injection
â”‚   â”‚
â”‚   â”œâ”€â”€ mcp/
â”‚   â”‚   â”œâ”€â”€ config.ts             # Reads ~/.claude/settings.json mcpServers
â”‚   â”‚   â”œâ”€â”€ manager.ts            # Connect/discover/call/disconnect MCP servers
â”‚   â”‚   â””â”€â”€ index.ts              # Barrel export
â”‚   â”‚
â”‚   â”œâ”€â”€ permissions/
â”‚   â”‚   â””â”€â”€ store.ts              # Pending approval store with 5-min expiry
â”‚   â”œâ”€â”€ audit/
â”‚   â”‚   â””â”€â”€ logger.ts             # Append-only JSONL event logger
â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”œâ”€â”€ parser.ts             # /command tokenizer
â”‚   â”‚   â””â”€â”€ handlers.ts           # Handler for each command
â”‚   â””â”€â”€ storage/
â”‚       â””â”€â”€ persistence.ts        # JSON/JSONL read-write helpers
â”‚
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â””â”€â”€ CLAUDE.md                     # Architecture and developer reference
```

### User data directories (`~/.safeclaw/`)

```
~/.safeclaw/
â”œâ”€â”€ auth.json                     # Stored API keys (owner-only file permissions)
â”œâ”€â”€ audit.jsonl                   # Append-only audit log
â”œâ”€â”€ soul.md                       # Optional custom persona (injected on wake)
â”œâ”€â”€ memories/                     # Persistent agent memory (key-value store)
â”œâ”€â”€ prompt-skills/                # SKILL.md files â€” teach LLM CLI patterns
â”‚   â”œâ”€â”€ weather.md
â”‚   â”œâ”€â”€ github.md
â”‚   â””â”€â”€ ...
â””â”€â”€ skills/                       # Dynamically installed JS skills
    â”œâ”€â”€ pdf_create.mjs
    â””â”€â”€ ...
```

---

## What's Implemented

| Feature | Status |
|---------|--------|
| Gateway state machine (dormant/awake/action_pending/shutdown) | âœ… |
| Single-owner auth with silent drop | âœ… |
| Runtime tool enable/disable | âœ… |
| `/confirm` dangerous action flow with 5-min expiry | âœ… |
| JSONL audit log | âœ… |
| Telegram bot (grammy) | âœ… |
| LLM agent â€” Anthropic, OpenAI, Gemini, Ollama | âœ… |
| Ollama native `/api/chat` with schema normalisation | âœ… |
| Live model listing from provider APIs | âœ… |
| Persistent API key storage | âœ… |
| Real filesystem tool (sandboxed to `WORKSPACE_DIR`) | âœ… |
| Real browser tool (fetch + Readability extraction) | âœ… |
| Real shell tool (`child_process`, 30s timeout) | âœ… |
| Apply-patch tool (Add/Update/Delete/Move) | âœ… |
| MCP auto-discovery from `~/.claude/settings.json` | âœ… |
| Self-extending dynamic skills (SkillCreator + security review) | âœ… |
| Soul file â€” custom persona | âœ… |
| Prompt skills â€” SKILL.md files injected into system prompt | âœ… |
| URL auto-enrichment â€” auto-fetch URLs in messages | âœ… |
| Message debouncing â€” merge burst messages | âœ… |
| Context window guard â€” truncate large tool results | âœ… |
| Auto-compaction â€” LLM summarises old history | âœ… |
| Background process execution â€” `exec_shell_bg` + poll/write/kill | âœ… |
| Persistent memory across sessions | âœ… |
| **SecretGuard â€” LLM blocked from reading secrets** | âœ… |
| **Infrastructure probe â€” CPU/RAM/GPU/Ollama on wake** | âœ… |
| **Multi-agent orchestration â€” manager/worker/reviewer pipeline** | âœ… |
| **SkillCreator agent â€” dedicated skill writing + security review** | âœ… |

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Runtime | Node.js 22+ |
| Language | TypeScript 5 (strict mode) |
| Telegram | [grammy](https://grammy.dev) |
| LLM: Anthropic | Anthropic Messages API (raw fetch) |
| LLM: OpenAI | OpenAI Chat Completions API (raw fetch) |
| LLM: Gemini | Google Generative Language API (raw fetch) |
| LLM: Ollama | Ollama native `/api/chat` (raw fetch, no wrapper library) |
| Browser | `@mozilla/readability` + `linkedom` |
| MCP | `@modelcontextprotocol/sdk` ^1.12.0 |
| Storage | File-based JSON + JSONL (no database) |
| Dev runner | `tsx` (TypeScript execute, no build step needed) |

---

## Roadmap

- [x] Gateway state machine, Telegram integration, commands, audit
- [x] Real filesystem tools with path sandboxing
- [x] LLM agent â€” Anthropic, OpenAI, Gemini, Ollama â€” with tool calling
- [x] Ollama native API + schema normalisation for small models
- [x] Live model listing from provider APIs
- [x] MCP tool auto-discovery from `~/.claude/settings.json`
- [x] Self-extending skills â€” SkillCreator agent + security Reviewer
- [x] Real browser tool (fetch + Readability)
- [x] Real shell execution (with timeout and output limits)
- [x] Apply-patch tool for code editing
- [x] Soul file â€” custom persona without code changes
- [x] Prompt skills â€” SKILL.md files teach CLI patterns
- [x] URL auto-enrichment in messages
- [x] Message debouncing
- [x] Context window guard
- [x] Auto-compaction of conversation history
- [x] Background process execution with poll/write/kill
- [x] Persistent memory across sessions
- [x] SecretGuard â€” LLM can never read API keys or `.env` files
- [x] Infrastructure probe â€” hardware-aware orchestration
- [x] Multi-agent orchestration â€” manager/worker/reviewer pipeline
- [ ] HTTP/SSE MCP server support
- [ ] WhatsApp Cloud API integration
- [ ] Web dashboard for visual tool management
- [ ] Multi-owner support with invite codes
- [ ] Semantic memory (SQLite + vector search across sessions)

---

## Contributing

Contributions welcome! Please open an issue first to discuss what you'd like to change. The architecture is documented in `CLAUDE.md`.

## License

[MIT](LICENSE)
